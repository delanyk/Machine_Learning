{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "with open('games-train.csv','r', newline='') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    for row in raw:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Extracting the Dataset\n",
    "data_set=[]\n",
    "for num, dat in enumerate(data):\n",
    "    dat[3] = re.sub(r'[^\\w\\säöüß]', ' ', dat[3].lower())\n",
    "    data_set.append([num, dat[1],[token for token in dat[3].lower().split()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "    def __init__(self):\n",
    "        self.c_prob = None\n",
    "        self.feature_types={}\n",
    "        self.class_term_freq ={} \n",
    "        self.token_counts = {}\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.idf = {}\n",
    "        \n",
    "    def train_test_split(self,dataset, split=0.85, random_state=3):\n",
    "        random.seed(random_state)\n",
    "        training_size = int(len(dataset)*split)\n",
    "        training_set = []\n",
    "        testing = dataset.copy()\n",
    "        while len(training_set) < training_size:\n",
    "            index = random.randrange(len(testing))\n",
    "            training_set.append(testing.pop(index))\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        for i in training_set:\n",
    "            x_train.append([i[0], i[2]])\n",
    "            y_train.append([i[0], i[1]])\n",
    "        for i in testing:\n",
    "            x_test.append([i[0], i[2]])\n",
    "            y_test.append([i[0], i[1]])\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    def _idf(self,data):\n",
    "        for line in data:\n",
    "            for term in line[1]:\n",
    "                self.vocab[term] += 1\n",
    "        idf= self.vocab.copy()\n",
    "        for term in idf.keys():\n",
    "            idf[term]= log10(len(data)/idf[term])\n",
    "        self.idf = idf\n",
    "        return idf           \n",
    "    \n",
    "    def train(self,data,labels):\n",
    "        idf = self._idf(data)\n",
    "        \n",
    "        classes = defaultdict(int)\n",
    "        features = self.feature_types\n",
    "        class_terms = self.class_term_freq\n",
    "        \n",
    "        for item in labels:\n",
    "            classes[item[1]] +=1\n",
    "        c_prob = {}\n",
    "        for key, val in classes.items():\n",
    "            c_prob[key] = val/len(labels)\n",
    "        self.c_prob = c_prob\n",
    "        \n",
    "        for key in c_prob.keys():\n",
    "            feature_count = 0\n",
    "            terms = defaultdict(int)\n",
    "            for num, item in enumerate(data):\n",
    "                if labels[num][1] == key:\n",
    "                    for word in item[1]:\n",
    "                            feature_count +=1\n",
    "                            terms[word] += 1\n",
    "         \n",
    "            \n",
    "            self.token_counts[key] = feature_count\n",
    "            for term,val in terms.items():\n",
    "                terms[term] = val+1\n",
    "            class_terms[key] = terms\n",
    "\n",
    "        for key in self.c_prob.keys():\n",
    "            weights = {}\n",
    "            for term, val in class_terms[key].items():\n",
    "                weights[term] = (val/(self.token_counts[key]))\n",
    "            self.feature_types[key] = weights\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def predict(self,train):\n",
    "        predictions = {}\n",
    "        for item in train:\n",
    "            c_pred=[]\n",
    "            total=1\n",
    "            for key in self.c_prob.keys():\n",
    "                for word in item[1]:\n",
    "                    try:\n",
    "                        total = total*self.feature_types[key][word]\n",
    "                    except:\n",
    "                        total = total*(1/(self.token_counts[key]+len(self.vocab)))\n",
    "                        continue\n",
    "                c_pred.append((total*self.c_prob[key],key))\n",
    "            predictions[item[0]] = sorted(c_pred, reverse=True)[0][1]\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def evaluate(self,pred,test):\n",
    "        def _cl_count(test):\n",
    "            classes=defaultdict(int)\n",
    "            for item in test:\n",
    "                classes[item[1]] +=1\n",
    "            return classes\n",
    "        \n",
    "        def _correct(pred,test, classes):\n",
    "            evals={}\n",
    "            for key,val in classes.items():\n",
    "                scores = {}\n",
    "                TP = 0\n",
    "                FP = 0\n",
    "                FN = 0\n",
    "                TN = 0\n",
    "                for item in test:\n",
    "                    if pred[item[0]] == key and item[1] == key:\n",
    "                        TP += 1 \n",
    "                    elif pred[item[0]] == key and item[1] != key:\n",
    "                        FP += 1\n",
    "                    elif pred[item[0]] != key and item[1] == key:\n",
    "                        FN += 1\n",
    "                    else:\n",
    "                        TN += 1     \n",
    "                scores['P'] = TP/(TP+FP)\n",
    "                scores['R'] = TP/(TP+FN)\n",
    "                scores['F1']= (2*scores['P']*scores['R'])/(scores['P']+scores['R'])\n",
    "                evals[key]=[scores, TP, FP, FN, TN]\n",
    "                print(\"Class:\", key, '\\n','TP:', TP, 'FP:', FP, 'FN:', FN,\n",
    "                      '\\n', 'Precision:', scores['P'], 'Recall:', scores['R'],\n",
    "                      '\\n',\"F1-score:\", scores['F1'] )\n",
    "            return evals\n",
    "            \n",
    "        def macro_f1(evals):\n",
    "            mac_P = 0\n",
    "            mac_R = 0\n",
    "            for key in evals.keys():\n",
    "                mac_P += evals[key][0]['P']\n",
    "                mac_R += evals[key][0]['R']\n",
    "            mac_P = mac_P/len(evals.keys())\n",
    "            mac_R = mac_R/len(evals.keys())\n",
    "            mac_F1 = (2*mac_P*mac_R)/(mac_P+mac_R)\n",
    "            return mac_P, mac_R, mac_F1\n",
    "\n",
    "        def micro_f1(evals):\n",
    "            TP=0\n",
    "            FP=0\n",
    "            FN=0\n",
    "            for item in  evals.values():\n",
    "                TP += item[1]\n",
    "                FP += item[2]\n",
    "                FN += item[3]\n",
    "            mic_P = TP/(TP+FP)\n",
    "            mic_R = TP/(TP+FN)\n",
    "            mic_F1 = (2*mic_P*mic_R)/(mic_P+mic_R)\n",
    "            return mic_P, mic_R, mic_F1\n",
    "        \n",
    "        classes = _cl_count(test)\n",
    "        evals = _correct(pred,test,classes)\n",
    "        mac_P,mac_R, mac_F1 = macro_f1(evals)\n",
    "        mic_P,mic_R, mic_F1 = micro_f1(evals)\n",
    "        \n",
    "        print(\"Macro Precision:\",mac_P,'\\tMacro Recall:',mac_R,'\\tMacro F1 measure:',mac_F1)\n",
    "        print(\"Micro Precision:\",mic_P,'\\tMicro Recall:',mic_R,'\\tMicro F1 measure:',mic_F1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Naive_Bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = clf.train_test_split(data_set,0.90, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 111443 \n",
      " test: 12383\n"
     ]
    }
   ],
   "source": [
    "print('train:', len(x_train),\"\\n\",\"test:\",len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gut: 111328 schlecht: 115\n"
     ]
    }
   ],
   "source": [
    "gut=0\n",
    "schlecht=0\n",
    "for val in pred.values():\n",
    "    if val == 'gut':\n",
    "        gut += 1\n",
    "    else:\n",
    "        schlecht += 1\n",
    "print('gut:',gut,'schlecht:',schlecht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: gut \n",
      " TP: 91681 FP: 19647 FN: 37 \n",
      " Precision: 0.8235214860592124 Recall: 0.9995965895462178 \n",
      " F1-score: 0.9030564502625021\n",
      "Class: schlecht \n",
      " TP: 78 FP: 37 FN: 19647 \n",
      " Precision: 0.6782608695652174 Recall: 0.003954372623574145 \n",
      " F1-score: 0.007862903225806453\n",
      "Macro Precision: 0.7508911778122149 \tMacro Recall: 0.501775481084896 \tMacro F1 measure: 0.6015627211166565\n",
      "Micro Precision: 0.8233715890634674 \tMicro Recall: 0.8233715890634674 \tMicro F1 measure: 0.8233715890634674\n"
     ]
    }
   ],
   "source": [
    "evals = clf.evaluate(pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Important Words (Top 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.034387872576877775, 'spiel'),\n",
       " (0.030234265648949786, 'es'),\n",
       " (0.028848990601424537, 'das'),\n",
       " (0.02636680841058266, 'ist'),\n",
       " (0.025264355730646632, 'ich'),\n",
       " (0.02162648370832252, 'und'),\n",
       " (0.016435861432567384, 'cool'),\n",
       " (0.013791749572439126, 'macht'),\n",
       " (0.013160667505151803, 'aber'),\n",
       " (0.012553985798989826, 'die'),\n",
       " (0.012009414102965087, 'man'),\n",
       " (0.011617899217635693, 'super'),\n",
       " (0.010763886578248632, 'nicht'),\n",
       " (0.010089549325330014, 'geil'),\n",
       " (0.010008584490686825, 'nur')]"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sorted([(val,key) for key, val in clf.feature_types['gut'].items()], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.029711842254933597, 'ich'),\n",
       " (0.028883095658714258, 'das'),\n",
       " (0.026619712097611027, 'nicht'),\n",
       " (0.024474720907396275, 'es'),\n",
       " (0.023968651557323962, 'und'),\n",
       " (0.02003848912763394, 'spiel'),\n",
       " (0.015196009016391539, 'ist'),\n",
       " (0.01395172841254402, 'mehr'),\n",
       " (0.012986018093139975, 'die'),\n",
       " (0.009603710556188787, 'man'),\n",
       " (0.008958356063894737, 'aber'),\n",
       " (0.008777285379006478, 'kann'),\n",
       " (0.00841746542826699, 'wieder'),\n",
       " (0.00757479031782548, 'nur'),\n",
       " (0.007423898080418598, 'auf')]"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(val,key) for key, val in clf.feature_types['schlecht'].items()], reverse =True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: gut \n",
      " TP: 10198 FP: 2176 FN: 4 \n",
      " Precision: 0.8241474058509779 Recall: 0.9996079200156832 \n",
      " F1-score: 0.9034372785258683\n",
      "Class: schlecht \n",
      " TP: 5 FP: 4 FN: 2176 \n",
      " Precision: 0.5555555555555556 Recall: 0.0022925263640531865 \n",
      " F1-score: 0.0045662100456621\n",
      "Macro Precision: 0.6898514807032667 \tMacro Recall: 0.5009502231898681 \tMacro F1 measure: 0.5804178010433477\n",
      "Micro Precision: 0.823952192522006 \tMicro Recall: 0.823952192522006 \tMicro F1 measure: 0.823952192522006\n"
     ]
    }
   ],
   "source": [
    "clf.evaluate(t_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Extracting Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "with open('games-test.csv','r', newline='') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    for row in raw:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "test = []\n",
    "for num, dat in enumerate(data):\n",
    "    dat[3] = re.sub(r'[^\\w\\säöüß]', ' ', dat[3].lower())\n",
    "    labels.append((num, dat[1]))\n",
    "    test.append((num,[token for token in dat[3].lower().split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gut: 44187 schlecht: 46\n"
     ]
    }
   ],
   "source": [
    "gut=0\n",
    "schlecht=0\n",
    "for val in test_pred.values():\n",
    "    if val == 'gut':\n",
    "        gut += 1\n",
    "    else:\n",
    "        schlecht += 1\n",
    "print('gut:',gut,'schlecht:',schlecht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: schlecht \n",
      " TP: 26 FP: 20 FN: 7791 \n",
      " Precision: 0.5652173913043478 Recall: 0.0033260841755149034 \n",
      " F1-score: 0.006613251939463309\n",
      "Class: gut \n",
      " TP: 36396 FP: 7791 FN: 20 \n",
      " Precision: 0.8236811731957363 Recall: 0.99945079086116 \n",
      " F1-score: 0.9030929369874572\n",
      "Macro Precision: 0.6944492822500421 \tMacro Recall: 0.5013884375183374 \tMacro F1 measure: 0.5823346007692746\n",
      "Micro Precision: 0.8234123844188728 \tMicro Recall: 0.8234123844188728 \tMicro F1 measure: 0.8234123844188728\n"
     ]
    }
   ],
   "source": [
    "clf.evaluate(test_pred, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
